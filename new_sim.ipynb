{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.sparse as sp\n",
    "import scipy.stats as stats\n",
    "from scipy.optimize import minimize\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import nbinom, poisson, bernoulli\n",
    "\n",
    "import pandas as pd\n",
    "import anndata\n",
    "import scanpy as sc\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata\n",
    "import scipy.sparse as sp\n",
    "from scipy.spatial.distance import cdist\n",
    "import scanpy as sc\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import anndata as ad\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata\n",
    "import scipy.sparse as sp\n",
    "from scipy.spatial.distance import cdist\n",
    "import scanpy as sc\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import genpareto\n",
    "import scanpy as sc\n",
    "from scipy import stats, optimize\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import anndata\n",
    "import scipy.sparse as sp\n",
    "from scipy.optimize import minimize_scalar\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.stats import poisson, nbinom, bernoulli\n",
    "from multiprocessing import Pool\n",
    "import anndata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simulate_gene_average_expression(adata, pseudocount=1, n_simulations=1000):\n",
    "    if sp.issparse(adata.X):\n",
    "        X = adata.X.toarray()\n",
    "    else:\n",
    "        X = adata.X\n",
    "    gene_totals = X.sum(axis=0)\n",
    "    gene_totals_pseudo = gene_totals + pseudocount\n",
    "    total_reads = gene_totals_pseudo.sum()\n",
    "    gene_probs = gene_totals_pseudo / total_reads\n",
    "    simulated_totals = np.random.multinomial(int(total_reads), gene_probs, size=n_simulations)\n",
    "    average_simulated_expression = simulated_totals.mean(axis=0)\n",
    "    n_cells = X.shape[0]\n",
    "    average_expression = average_simulated_expression / n_cells\n",
    "    return dict(zip(adata.var_names, average_expression))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础模型部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型拟合部分：mean，var拟合以及基因模型拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse as sp\n",
    "from scipy.stats import genpareto, ks_2samp\n",
    "\n",
    "class MeanSimulator:\n",
    "    def __init__(self, mode=\"strict\", threshold=0.99):\n",
    "        self.mode = mode\n",
    "        self.threshold = threshold\n",
    "        self.shape = None\n",
    "        self.loc = None\n",
    "        self.scale = None\n",
    "        self.best_threshold = None\n",
    "        self.tail_data = None\n",
    "        self.original_order = None\n",
    "        self.best_evaluation = None\n",
    "        self.original_data = None\n",
    "\n",
    "    def extract_data(self, X):\n",
    "        return np.mean(X, axis=0)\n",
    "\n",
    "    def fit(self, adata):\n",
    "        if sp.issparse(adata.X):\n",
    "            X = adata.X.toarray()\n",
    "        else:\n",
    "            X = adata.X\n",
    "        \n",
    "        self.original_data = self.extract_data(X)\n",
    "        self.original_order = np.argsort(self.original_data)\n",
    "        sorted_data = np.sort(self.original_data)\n",
    "\n",
    "        if self.mode == \"strict\":\n",
    "            thresholds = [99.5, 99, 98.5, 98, 97.5, 97,96,95]\n",
    "            best_score = float('inf')\n",
    "\n",
    "            for percentile in thresholds:\n",
    "                current_threshold = np.percentile(sorted_data, percentile)\n",
    "                main_data = sorted_data[sorted_data <= current_threshold]\n",
    "                self.tail_data = sorted_data[sorted_data > current_threshold]\n",
    "\n",
    "                shape, loc, scale = genpareto.fit(main_data)\n",
    "\n",
    "                n_main = len(main_data)\n",
    "                n_tail = len(self.tail_data)\n",
    "\n",
    "                new_main = genpareto.rvs(shape, loc, scale, size=n_main)\n",
    "                new_tail = np.random.choice(self.tail_data, size=n_tail, replace=True)\n",
    "\n",
    "                new_samples = np.concatenate([new_main, new_tail])\n",
    "                new_samples = np.clip(new_samples, np.min(self.original_data), np.max(self.original_data))\n",
    "\n",
    "                evaluation = self.evaluate_fit(self.original_data, new_samples)\n",
    "\n",
    "                score = (abs(evaluation[\"Cohen's d\"]) + \n",
    "                         evaluation[\"Relative Error\"] + \n",
    "                         evaluation[\"KS Statistic\"] + \n",
    "                         (1 - evaluation[\"Correlation\"]))\n",
    "\n",
    "                if score < best_score:\n",
    "                    best_score = score\n",
    "                    self.best_threshold = percentile\n",
    "                    self.shape, self.loc, self.scale = shape, loc, scale\n",
    "                    self.best_evaluation = evaluation\n",
    "\n",
    "            print(f\"Best threshold: {self.best_threshold}\")\n",
    "            print(f\"Best shape: {self.shape}\")\n",
    "            print(f\"Best loc: {self.loc}\")\n",
    "            print(f\"Best scale: {self.scale}\")\n",
    "\n",
    "        else:\n",
    "            threshold_value = np.percentile(sorted_data, 99)\n",
    "            main_data = sorted_data[sorted_data <= threshold_value]\n",
    "            self.tail_data = sorted_data[sorted_data > threshold_value]\n",
    "\n",
    "            self.shape, self.loc, self.scale = genpareto.fit(main_data)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def simulate(self, n_samples):\n",
    "        if self.mode == \"strict\":\n",
    "            n_main = int(n_samples * self.best_threshold / 100)\n",
    "        else:\n",
    "            n_main = int(n_samples * 0.99)  # 使用99%作为非严格模式的默认阈值\n",
    "        n_tail = n_samples - n_main\n",
    "\n",
    "        new_main = genpareto.rvs(self.shape, loc=self.loc, scale=self.scale, size=n_main)\n",
    "        new_tail = np.random.choice(self.tail_data, size=n_tail, replace=True)\n",
    "\n",
    "        new_samples = np.concatenate([new_main, new_tail])\n",
    "        new_samples = np.clip(new_samples, np.min(self.original_data), np.max(self.original_data))\n",
    "        new_samples = np.sort(new_samples)\n",
    "        \n",
    "        # 确保模拟的数据保持原来的排序\n",
    "        simulated_data = np.zeros_like(new_samples)\n",
    "        simulated_data[self.original_order] = new_samples\n",
    "        \n",
    "        return simulated_data\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate_fit(original, generated, quantiles=[0.25, 0.5, 0.75, 0.9, 0.95, 0.99]):\n",
    "        def cohens_d(x1, x2):\n",
    "            n1, n2 = len(x1), len(x2)\n",
    "            var1, var2 = np.var(x1, ddof=1), np.var(x2, ddof=1)\n",
    "            pooled_se = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))\n",
    "            return (np.mean(x1) - np.mean(x2)) / pooled_se\n",
    "\n",
    "        def relative_error(x1, x2):\n",
    "            return np.abs(np.mean(x1) - np.mean(x2)) / np.mean(x1)\n",
    "\n",
    "        # 计算主要指标\n",
    "        effect_size = cohens_d(original, generated)\n",
    "        rel_error = relative_error(original, generated)\n",
    "        ks_stat, _ = ks_2samp(original, generated)\n",
    "        correlation = np.corrcoef(np.sort(original), np.sort(generated))[0, 1]\n",
    "\n",
    "        # 计算分位数相对误差\n",
    "        orig_quant = np.quantile(original, quantiles)\n",
    "        gen_quant = np.quantile(generated, quantiles)\n",
    "        quant_rel_errors = np.abs(orig_quant - gen_quant) / orig_quant\n",
    "\n",
    "        # 评估结果\n",
    "        results = {\n",
    "            \"Cohen's d\": effect_size,\n",
    "            \"Relative Error\": rel_error,\n",
    "            \"KS Statistic\": ks_stat,\n",
    "            \"Correlation\": correlation,\n",
    "            \"Quantile Relative Errors\": dict(zip([f\"{q*100}th\" for q in quantiles], quant_rel_errors))\n",
    "        }\n",
    "\n",
    "        # 修改判定标准\n",
    "        excellent = (abs(effect_size) < 0.05 and rel_error < 0.05 and ks_stat < 0.1 and correlation > 0.95)\n",
    "        good = (abs(effect_size) < 0.1 and rel_error < 0.15 and ks_stat < 0.15 and correlation > 0.9)\n",
    "        fair = (abs(effect_size) < 0.2 and rel_error < 0.2 and ks_stat < 0.2 and correlation > 0.8)\n",
    "\n",
    "        if excellent:\n",
    "            verdict = \"Excellent fit\"\n",
    "        elif good:\n",
    "            verdict = \"Good fit\"\n",
    "        elif fair:\n",
    "            verdict = \"Fair fit\"\n",
    "        else:\n",
    "            verdict = \"Poor fit\"\n",
    "\n",
    "        results[\"Verdict\"] = verdict\n",
    "\n",
    "        return results\n",
    "\n",
    "def simulate_gene_means(adata, mode=\"strict\", threshold=0.99):\n",
    "    simulator = MeanSimulator(mode=mode, threshold=threshold)\n",
    "    simulator.fit(adata)\n",
    "    simulated_values = simulator.simulate(adata.n_vars)\n",
    "    \n",
    "    result_dict = dict(zip(adata.var_names, simulated_values))\n",
    "    \n",
    "    if mode == \"strict\":\n",
    "        return result_dict, simulator.best_threshold, simulator.best_evaluation\n",
    "    else:\n",
    "        return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse as sp\n",
    "from scipy.stats import invgamma, ks_2samp\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import gammaln\n",
    "\n",
    "class IG_VarianceSimulator:\n",
    "    def __init__(self):\n",
    "        self.alpha = None\n",
    "        self.beta = None\n",
    "        self.threshold = None\n",
    "        self.tail_data = None\n",
    "        self.original_order = None\n",
    "        self.original_data = None\n",
    "        self.random_state = None\n",
    "\n",
    "    def extract_data(self, X):\n",
    "        variances = np.var(X, axis=0, ddof=1)\n",
    "        variances = np.nan_to_num(variances, nan=np.nanmean(variances))\n",
    "        variances = np.maximum(variances, 1e-10)\n",
    "        return variances\n",
    "\n",
    "    def assess_tail_discreteness(self, tail_data):\n",
    "        sorted_tail = np.sort(tail_data)\n",
    "        differences = np.diff(sorted_tail)\n",
    "        cv = np.std(differences) / np.mean(differences)\n",
    "\n",
    "        if cv > 2.0:\n",
    "            return 'discrete'\n",
    "        elif cv < 1.0:\n",
    "            return 'smooth'\n",
    "        else:\n",
    "            return 'mixed'\n",
    "\n",
    "    def fit_single_component(self, data):\n",
    "        try:\n",
    "            params = invgamma.fit(data, floc=0)\n",
    "            return params[0], params[2]\n",
    "        except Exception as e:\n",
    "            print(f\"Single component fitting failed: {str(e)}\")\n",
    "            return 1.0, np.mean(data)  # Fallback values\n",
    "\n",
    "    def fit(self, adata):\n",
    "        try:\n",
    "            if sp.issparse(adata.X):\n",
    "                X = adata.X.toarray()\n",
    "            else:\n",
    "                X = adata.X\n",
    "\n",
    "            self.original_data = self.extract_data(X)\n",
    "            if not np.all(np.isfinite(self.original_data)):\n",
    "                raise ValueError(\"Data contains non-finite values\")\n",
    "\n",
    "            self.original_order = np.argsort(self.original_data)\n",
    "            sorted_data = np.sort(self.original_data)\n",
    "\n",
    "            thresholds = [99.5]\n",
    "            best_score = float('inf')\n",
    "            best_evaluation = None\n",
    "            for percentile in thresholds:\n",
    "                current_threshold = np.percentile(sorted_data, percentile)\n",
    "                main_data = sorted_data[sorted_data <= current_threshold]\n",
    "                tail_data = sorted_data[sorted_data > current_threshold]\n",
    "\n",
    "                if len(main_data) == 0 or len(tail_data) == 0:\n",
    "                    print(f\"Warning: Empty main_data or tail_data at threshold {percentile}\")\n",
    "                    continue\n",
    "\n",
    "                alpha, beta = self.fit_single_component(main_data)\n",
    "\n",
    "                n_main = len(main_data)\n",
    "                n_tail = len(tail_data)\n",
    "\n",
    "                new_main = invgamma.rvs(alpha, scale=beta, size=n_main, random_state=self.random_state)\n",
    "\n",
    "                tail_type = self.assess_tail_discreteness(tail_data)\n",
    "                if tail_type == 'discrete':\n",
    "                    new_tail = self.random_state.choice(tail_data, size=n_tail, replace=True)\n",
    "                else:\n",
    "                    new_tail = np.interp(\n",
    "                        np.linspace(0, 1, n_tail),\n",
    "                        np.linspace(0, 1, len(tail_data)),\n",
    "                        np.sort(tail_data)\n",
    "                    )\n",
    "\n",
    "                new_samples = np.concatenate([new_main, new_tail])\n",
    "                new_samples = np.clip(new_samples, np.min(self.original_data), np.max(self.original_data))\n",
    "\n",
    "                evaluation = self.evaluate_fit(self.original_data, new_samples)\n",
    "\n",
    "                score = (abs(evaluation[\"Cohen's d\"]) + \n",
    "                         evaluation[\"Relative Error\"] + \n",
    "                         evaluation[\"KS Statistic\"] + \n",
    "                         (1 - evaluation[\"Correlation\"]))\n",
    "\n",
    "                if score < best_score:\n",
    "                    best_score = score\n",
    "                    self.threshold = percentile\n",
    "                    self.alpha, self.beta = alpha, beta\n",
    "                    best_evaluation = evaluation\n",
    "                    self.tail_data = tail_data\n",
    "\n",
    "            if self.threshold is None:\n",
    "                print(\"Warning: No valid threshold found, fallback to default.\")\n",
    "                self.threshold = 95  # Default threshold\n",
    "\n",
    "            return self, best_evaluation\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Fitting failed: {str(e)}\")\n",
    "            self.alpha, self.beta = self.fit_single_component(self.original_data)\n",
    "            return self, {\"Verdict\": \"Fallback to single component\"}\n",
    "\n",
    "    def simulate(self, n_samples):\n",
    "        try:\n",
    "            if self.threshold is None:\n",
    "                print(\"Warning: Threshold is None, using default value of 95.\")\n",
    "                self.threshold = 95\n",
    "\n",
    "            n_main = int(n_samples * self.threshold / 100)\n",
    "            n_tail = n_samples - n_main\n",
    "\n",
    "            new_main = invgamma.rvs(self.alpha, scale=self.beta, size=n_main, random_state=self.random_state)\n",
    "\n",
    "            tail_type = self.assess_tail_discreteness(self.tail_data)\n",
    "            if tail_type == 'discrete':\n",
    "                new_tail = self.random_state.choice(self.tail_data, size=n_tail, replace=True)\n",
    "            else:\n",
    "                new_tail = np.interp(\n",
    "                    np.linspace(0, 1, n_tail),\n",
    "                    np.linspace(0, 1, len(self.tail_data)),\n",
    "                    np.sort(self.tail_data)\n",
    "                )\n",
    "\n",
    "            new_samples = np.concatenate([new_main, new_tail])\n",
    "            new_samples = np.clip(new_samples, np.min(self.original_data), np.max(self.original_data))\n",
    "            new_samples = np.sort(new_samples)\n",
    "\n",
    "            simulated_data = np.zeros_like(new_samples)\n",
    "            simulated_data[self.original_order] = new_samples\n",
    "\n",
    "            return simulated_data\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Simulation failed: {str(e)}\")\n",
    "            return self.random_state.choice(self.original_data, size=n_samples, replace=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate_fit(original, generated, quantiles=[0.25, 0.5, 0.75, 0.9, 0.95, 0.99, 1]):\n",
    "        \"\"\"评估生成的数据与原始数据的拟合度\"\"\"\n",
    "        def cohens_d(x1, x2):\n",
    "            n1, n2 = len(x1), len(x2)\n",
    "            var1, var2 = np.var(x1, ddof=1), np.var(x2, ddof=1)\n",
    "            pooled_se = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))\n",
    "            return (np.mean(x1) - np.mean(x2)) / pooled_se\n",
    "\n",
    "        def relative_error(x1, x2):\n",
    "            return np.abs(np.mean(x1) - np.mean(x2)) / np.mean(x1)\n",
    "\n",
    "        effect_size = cohens_d(original, generated)\n",
    "        rel_error = relative_error(original, generated)\n",
    "        ks_stat, _ = ks_2samp(original, generated)\n",
    "        correlation = np.corrcoef(np.sort(original), np.sort(generated))[0, 1]\n",
    "\n",
    "        orig_quant = np.quantile(original, quantiles)\n",
    "        gen_quant = np.quantile(generated, quantiles)\n",
    "        quant_rel_errors = np.abs(orig_quant - gen_quant) / orig_quant\n",
    "\n",
    "        results = {\n",
    "            \"Cohen's d\": effect_size,\n",
    "            \"Relative Error\": rel_error,\n",
    "            \"KS Statistic\": ks_stat,\n",
    "            \"Correlation\": correlation,\n",
    "            \"Quantile Relative Errors\": dict(zip([f\"{q*100}th\" for q in quantiles], quant_rel_errors))\n",
    "        }\n",
    "\n",
    "        excellent = (abs(effect_size) < 0.05 and rel_error < 0.05 and ks_stat < 0.1 and correlation > 0.95)\n",
    "        good = (abs(effect_size) < 0.1 and rel_error < 0.15 and ks_stat < 0.15 and correlation > 0.9)\n",
    "        fair = (abs(effect_size) < 0.2 and rel_error < 0.2 and ks_stat < 0.2 and correlation > 0.8)\n",
    "\n",
    "        if excellent:\n",
    "            verdict = \"Excellent fit\"\n",
    "        elif good:\n",
    "            verdict = \"Good fit\"\n",
    "        elif fair:\n",
    "            verdict = \"Fair fit\"\n",
    "        else:\n",
    "            verdict = \"Poor fit\"\n",
    "\n",
    "        results[\"Verdict\"] = verdict\n",
    "\n",
    "        return results\n",
    "\n",
    "    def fit_and_simulate(self, adata, n_iterations=30):\n",
    "        best_simulation = None\n",
    "        best_evaluation = None\n",
    "        best_score = float('inf')\n",
    "        best_threshold = None\n",
    "        best_alpha = None\n",
    "        best_beta = None\n",
    "\n",
    "        for _ in range(n_iterations):\n",
    "            self.random_state = np.random.RandomState()  # 每次迭代使用新的随机种子\n",
    "            self, evaluation = self.fit(adata)\n",
    "            simulated_values = self.simulate(adata.n_vars)\n",
    "            final_evaluation = self.evaluate_fit(self.original_data, simulated_values)\n",
    "\n",
    "            score = (abs(final_evaluation[\"Cohen's d\"]) + \n",
    "                     final_evaluation[\"Relative Error\"] + \n",
    "                     final_evaluation[\"KS Statistic\"] + \n",
    "                     (1 - final_evaluation[\"Correlation\"]))\n",
    "\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_simulation = simulated_values\n",
    "                best_evaluation = final_evaluation\n",
    "                best_threshold = self.threshold\n",
    "                best_alpha = self.alpha\n",
    "                best_beta = self.beta\n",
    "\n",
    "        self.threshold = best_threshold\n",
    "        self.alpha = best_alpha\n",
    "        self.beta = best_beta\n",
    "\n",
    "        return best_simulation, best_evaluation\n",
    "\n",
    "def simulate_gene_variances_advanced(adata, n_iterations=10):\n",
    "    simulator = IG_VarianceSimulator()\n",
    "    simulated_values, final_evaluation = simulator.fit_and_simulate(adata, n_iterations)\n",
    "\n",
    "    result_dict = dict(zip(adata.var_names, simulated_values))\n",
    "\n",
    "    print(\"\\nFinal evaluation:\")\n",
    "    print(final_evaluation)\n",
    "\n",
    "    # 添加更多诊断信息\n",
    "    print(\"\\nDiagnostic Information:\")\n",
    "    print(f\"Original data mean: {np.mean(simulator.original_data)}\")\n",
    "    print(f\"Original data variance: {np.var(simulator.original_data)}\")\n",
    "    print(f\"Simulated data mean: {np.mean(simulated_values)}\")\n",
    "    print(f\"Simulated data variance: {np.var(simulated_values)}\")\n",
    "    print(f\"Best threshold: {simulator.threshold}\")\n",
    "    print(f\"Best alpha: {simulator.alpha}\")\n",
    "    print(f\"Best beta: {simulator.beta}\")\n",
    "\n",
    "    return result_dict, simulator.threshold, final_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_zero_proportion(X):\n",
    "    return np.mean(X == 0, axis=0)\n",
    "\n",
    "def genpareto_fit_zero_p(filtered_adata, mode=\"strict\", threshold=0.99):\n",
    "    if sp.issparse(filtered_adata.X):\n",
    "        filtered_adata.X = filtered_adata.X.A\n",
    "\n",
    "    zero_proportion = calc_zero_proportion(filtered_adata.X)\n",
    "    zero_proportion = 1- zero_proportion\n",
    "\n",
    "    def transform_data(data, epsilon=1e-10):\n",
    "        return -np.log(1 - np.clip(data, 0, 1-epsilon))\n",
    "\n",
    "    def inverse_transform(data):\n",
    "        return 1 - np.exp(-data)\n",
    "\n",
    "    def fit_and_sample(data, n_samples):\n",
    "        transformed_data = transform_data(data)\n",
    "        shape, loc, scale = genpareto.fit(transformed_data)\n",
    "        samples = genpareto.rvs(shape, loc, scale, size=n_samples)\n",
    "        return inverse_transform(samples)\n",
    "\n",
    "    def evaluate_fit(original, new):\n",
    "        ks_stat, _ = ks_2samp(original, new)\n",
    "        return {\n",
    "            \"KS Statistic\": ks_stat,\n",
    "            \"Mean Difference\": np.mean(original) - np.mean(new),\n",
    "            \"Std Difference\": np.std(original) - np.std(new),\n",
    "            \"Correlation\": np.corrcoef(original, new)[0, 1]\n",
    "        }\n",
    "\n",
    "    if mode == \"strict\":\n",
    "        thresholds = [0.995, 0.99, 0.985, 0.98, 0.975]\n",
    "        best_score = float('inf')\n",
    "        best_threshold = None\n",
    "        best_samples = None\n",
    "        best_evaluation = None\n",
    "\n",
    "        for percentile in thresholds:\n",
    "            threshold = np.percentile(zero_proportion, percentile * 100)\n",
    "            main_data = zero_proportion[zero_proportion <= threshold]\n",
    "            tail_data = zero_proportion[zero_proportion > threshold]\n",
    "\n",
    "            n_main = len(main_data)\n",
    "            n_tail = len(tail_data)\n",
    "\n",
    "            new_main = fit_and_sample(main_data, n_main)\n",
    "            new_tail = np.random.choice(tail_data, size=n_tail, replace=True)\n",
    "\n",
    "            new_samples = np.concatenate([new_main, new_tail])\n",
    "            new_samples = np.clip(new_samples, 0, 1)\n",
    "\n",
    "            evaluation = evaluate_fit(zero_proportion, new_samples)\n",
    "\n",
    "            score = (abs(evaluation[\"Mean Difference\"]) + \n",
    "                     abs(evaluation[\"Std Difference\"]) + \n",
    "                     evaluation[\"KS Statistic\"] + \n",
    "                     (1 - evaluation[\"Correlation\"]))\n",
    "\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_threshold = percentile\n",
    "                best_samples = new_samples\n",
    "                best_evaluation = evaluation\n",
    "\n",
    "        original_order = np.argsort(zero_proportion)\n",
    "        best_samples = best_samples[original_order]\n",
    "\n",
    "        return 1-best_samples, best_threshold, best_evaluation\n",
    "\n",
    "    else:\n",
    "        threshold = np.percentile(zero_proportion, threshold * 100)\n",
    "        main_data = zero_proportion[zero_proportion <= threshold]\n",
    "        tail_data = zero_proportion[zero_proportion > threshold]\n",
    "\n",
    "        n_main = len(main_data)\n",
    "        n_tail = len(tail_data)\n",
    "\n",
    "        new_main = fit_and_sample(main_data, n_main)\n",
    "        new_tail = np.random.choice(tail_data, size=n_tail, replace=True)\n",
    "\n",
    "        new_samples = np.concatenate([new_main, new_tail])\n",
    "        new_samples = np.clip(new_samples, 0, 1)\n",
    "\n",
    "        original_order = np.argsort(zero_proportion)\n",
    "        new_samples = new_samples[original_order]\n",
    "\n",
    "        return 1-new_samples\n",
    "    \n",
    "\n",
    "\n",
    "def simulate_gene_pis(adata, mode=\"strict\", threshold=0.99):\n",
    "    simulated_values, best_threshold, best_evaluation = genpareto_fit_zero_p(adata, mode=mode, threshold=threshold)\n",
    "    \n",
    "    result_dict = dict(zip(adata.var_names, simulated_values))\n",
    "    \n",
    "    if mode == \"strict\":\n",
    "        return result_dict, best_threshold, best_evaluation\n",
    "    else:\n",
    "        return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_loglikelihood_fixed_mu(r, x, mu):\n",
    "    p = r / (r + mu)\n",
    "    return -np.sum(stats.nbinom.logpmf(x, r, p))\n",
    "\n",
    "def zinb_loglikelihood_fixed_mu(params, x, mu):\n",
    "    pi, r = params\n",
    "    p = r / (r + mu)\n",
    "    x_is_zero = (x == 0)\n",
    "    ll_zero = x_is_zero * np.log(pi + (1 - pi) * stats.nbinom.pmf(0, r, p))\n",
    "    ll_nonzero = ~x_is_zero * (np.log(1 - pi) + stats.nbinom.logpmf(x, r, p))\n",
    "    return -np.sum(ll_zero + ll_nonzero)\n",
    "\n",
    "def zip_loglikelihood_fixed_mu(pi, x, mu):\n",
    "    x_is_zero = (x == 0)\n",
    "    ll_zero = x_is_zero * np.log(pi + (1 - pi) * np.exp(-mu))\n",
    "    ll_nonzero = ~x_is_zero * (np.log(1 - pi) + stats.poisson.logpmf(x, mu))\n",
    "    return -np.sum(ll_zero + ll_nonzero)\n",
    "\n",
    "def poisson_loglikelihood(mu, x):\n",
    "    return -np.sum(stats.poisson.logpmf(x, mu))\n",
    "\n",
    "def fit_with_simulated_mean_and_var(gene, simulated_mean, simulated_var, maxiter=100):\n",
    "    mu = simulated_mean\n",
    "    \n",
    "    # 计算负二项分布参数\n",
    "    if simulated_var < simulated_mean:  # 如果方差小于均值，使用泊松分布\n",
    "        return [0, np.inf, mu, \"Poisson\"]\n",
    "    else:\n",
    "        p = (simulated_var - mu) / simulated_var\n",
    "        r = mu * (1 - p) / p\n",
    "        \n",
    "        # 计算各个模型的似然值\n",
    "        ll_nb = -nb_loglikelihood_fixed_mu(r, gene, mu)\n",
    "        \n",
    "        result_zinb = minimize(zinb_loglikelihood_fixed_mu, [0.5, r], \n",
    "                             args=(gene, mu), \n",
    "                             bounds=[(1e-6, 1-1e-6), (1e-6, 1e6)])\n",
    "        pi_zinb, r_zinb = result_zinb.x\n",
    "        ll_zinb = -result_zinb.fun\n",
    "\n",
    "        result_zip = minimize_scalar(zip_loglikelihood_fixed_mu, \n",
    "                                   args=(gene, mu), \n",
    "                                   bounds=(1e-6, 1-1e-6), \n",
    "                                   method='bounded')\n",
    "        pi_zip = result_zip.x\n",
    "        ll_zip = -result_zip.fun\n",
    "\n",
    "        ll_poisson = -poisson_loglikelihood(mu, gene)\n",
    "\n",
    "        # 计算AIC\n",
    "        aic_nb = 2 * 2 - 2 * ll_nb\n",
    "        aic_zinb = 2 * 3 - 2 * ll_zinb\n",
    "        aic_zip = 2 * 2 - 2 * ll_zip\n",
    "        aic_poisson = 2 * 1 - 2 * ll_poisson\n",
    "        \n",
    "        aics = [aic_nb, aic_zinb, aic_zip, aic_poisson]\n",
    "        best_model_idx = np.argmin(aics)\n",
    "\n",
    "        if best_model_idx == 0:\n",
    "            return [0, r, mu, \"NB\"]\n",
    "        elif best_model_idx == 1:\n",
    "            return [pi_zinb, r_zinb, mu, \"ZINB\"]\n",
    "        elif best_model_idx == 2:\n",
    "            return [pi_zip, np.inf, mu, \"ZIP\"]\n",
    "        else:\n",
    "            return [0, np.inf, mu, \"Poisson\"]\n",
    "\n",
    "def fit_marginal_model_with_simulated_params(adata, simulated_means, simulated_vars, \n",
    "                                           min_nonzero_num=2, maxiter=500, n_jobs=-1):\n",
    "    if not isinstance(adata, anndata.AnnData):\n",
    "        raise ValueError(\"Input adata should be an AnnData object\")\n",
    "    \n",
    "    if sp.issparse(adata.X):\n",
    "        x = adata.X.toarray()\n",
    "    else:\n",
    "        x = adata.X\n",
    "    \n",
    "    gene_names = adata.var_names.tolist()\n",
    "    n, p = x.shape\n",
    "    \n",
    "    if len(simulated_means) != p or len(simulated_vars) != p:\n",
    "        raise ValueError(\"Length of simulated parameters does not match number of genes\")\n",
    "    \n",
    "    gene_zero_prop = 1 - np.sum(x > 0, axis=0) / n\n",
    "    gene_sel1 = np.where(gene_zero_prop < 1 - min_nonzero_num / n)[0]\n",
    "    gene_sel2 = np.setdiff1d(np.arange(p), gene_sel1)\n",
    "    \n",
    "    if len(gene_sel1) == 0:\n",
    "        print(\"Warning: No genes selected for fitting models.\")\n",
    "        return None\n",
    "    \n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(fit_with_simulated_mean_and_var)(\n",
    "            x[:, i], \n",
    "            simulated_means[gene_names[i]], \n",
    "            simulated_vars[gene_names[i]], \n",
    "            maxiter\n",
    "        )\n",
    "        for i in tqdm(gene_sel1, desc=\"Fitting models\")\n",
    "    )\n",
    "    \n",
    "    params_df = pd.DataFrame(results, \n",
    "                           index=[gene_names[i] for i in gene_sel1], \n",
    "                           columns=['pi0', 'theta', 'mu', 'model_selected'])\n",
    "    \n",
    "    model_params = {\n",
    "        'gene_sel1': {i: gene_names[i] for i in gene_sel1},\n",
    "        'gene_sel2': {i: gene_names[i] for i in gene_sel2},\n",
    "        'marginal_param1': params_df[['pi0', 'theta', 'mu']].values.tolist(),\n",
    "        'model_selected': params_df['model_selected'].tolist(),\n",
    "        'min_nonzero_num': min_nonzero_num,\n",
    "        'n_cell': n,\n",
    "        'n_read': np.sum(x)\n",
    "    }\n",
    "    \n",
    "    return model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from scipy import stats\n",
    "# from scipy.optimize import minimize, minimize_scalar\n",
    "# import pandas as pd\n",
    "# from joblib import Parallel, delayed\n",
    "# from tqdm import tqdm\n",
    "# import anndata\n",
    "# from scipy import sparse as sp\n",
    "\n",
    "# def nb_loglikelihood_fixed_mu(r, x, mu):\n",
    "#     p = r / (r + mu)\n",
    "#     return -np.sum(stats.nbinom.logpmf(x, r, p))\n",
    "\n",
    "# def zinb_loglikelihood_fixed_mu(params, x, mu):\n",
    "#     pi, r = params\n",
    "#     p = r / (r + mu)\n",
    "#     x_is_zero = (x == 0)\n",
    "#     ll_zero = x_is_zero * np.log(pi + (1 - pi) * stats.nbinom.pmf(0, r, p))\n",
    "#     ll_nonzero = ~x_is_zero * (np.log(1 - pi) + stats.nbinom.logpmf(x, r, p))\n",
    "#     return -np.sum(ll_zero + ll_nonzero)\n",
    "\n",
    "# def zip_loglikelihood_fixed_mu(pi, x, mu):\n",
    "#     x_is_zero = (x == 0)\n",
    "#     ll_zero = x_is_zero * np.log(pi + (1 - pi) * np.exp(-mu))\n",
    "#     ll_nonzero = ~x_is_zero * (np.log(1 - pi) + stats.poisson.logpmf(x, mu))\n",
    "#     return -np.sum(ll_zero + ll_nonzero)\n",
    "\n",
    "# def poisson_loglikelihood(mu, x):\n",
    "#     return -np.sum(stats.poisson.logpmf(x, mu))\n",
    "\n",
    "# def fit_with_simulated_params(gene, simulated_mean, simulated_var, simulated_pi, maxiter=100):\n",
    "#     mu = simulated_mean\n",
    "#     pi = simulated_pi\n",
    "    \n",
    "#     # 计算负二项分布参数\n",
    "#     if simulated_var < simulated_mean:  # 如果方差小于均值，使用泊松分布\n",
    "#         return [pi, np.inf, mu, \"ZIP\"]\n",
    "#     else:\n",
    "#         p = (simulated_var - mu) / simulated_var\n",
    "#         r = mu * (1 - p) / p\n",
    "        \n",
    "#         # 计算各个模型的似然值\n",
    "#         ll_nb = -nb_loglikelihood_fixed_mu(r, gene, mu)\n",
    "        \n",
    "#         result_zinb = minimize(zinb_loglikelihood_fixed_mu, [pi, r], \n",
    "#                              args=(gene, mu), \n",
    "#                              bounds=[(1e-6, 1-1e-6), (1e-6, 1e6)])\n",
    "#         pi_zinb, r_zinb = result_zinb.x\n",
    "#         ll_zinb = -result_zinb.fun\n",
    "\n",
    "#         ll_zip = -zip_loglikelihood_fixed_mu(pi, gene, mu)\n",
    "\n",
    "#         ll_poisson = -poisson_loglikelihood(mu, gene)\n",
    "\n",
    "#         # 计算AIC\n",
    "#         aic_nb = 2 * 2 - 2 * ll_nb\n",
    "#         aic_zinb = 2 * 3 - 2 * ll_zinb\n",
    "#         aic_zip = 2 * 2 - 2 * ll_zip\n",
    "#         aic_poisson = 2 * 1 - 2 * ll_poisson\n",
    "        \n",
    "#         aics = [aic_nb, aic_zinb, aic_zip, aic_poisson]\n",
    "#         best_model_idx = np.argmin(aics)\n",
    "\n",
    "#         if best_model_idx == 0:\n",
    "#             return [0, r, mu, \"NB\"]\n",
    "#         elif best_model_idx == 1:\n",
    "#             return [pi_zinb, r_zinb, mu, \"ZINB\"]\n",
    "#         elif best_model_idx == 2:\n",
    "#             return [pi, np.inf, mu, \"ZIP\"]\n",
    "#         else:\n",
    "#             return [0, np.inf, mu, \"Poisson\"]\n",
    "\n",
    "# def fit_marginal_model_with_simulated_params(adata, simulated_means, simulated_vars, simulated_pis,\n",
    "#                                            min_nonzero_num=2, maxiter=500, n_jobs=-1):\n",
    "#     if not isinstance(adata, anndata.AnnData):\n",
    "#         raise ValueError(\"Input adata should be an AnnData object\")\n",
    "    \n",
    "#     if sp.issparse(adata.X):\n",
    "#         x = adata.X.toarray()\n",
    "#     else:\n",
    "#         x = adata.X\n",
    "    \n",
    "#     gene_names = adata.var_names.tolist()\n",
    "#     n, p = x.shape\n",
    "    \n",
    "#     if len(simulated_means) != p or len(simulated_vars) != p or len(simulated_pis) != p:\n",
    "#         raise ValueError(\"Length of simulated parameters does not match number of genes\")\n",
    "    \n",
    "#     gene_zero_prop = 1 - np.sum(x > 0, axis=0) / n\n",
    "#     gene_sel1 = np.where(gene_zero_prop < 1 - min_nonzero_num / n)[0]\n",
    "#     gene_sel2 = np.setdiff1d(np.arange(p), gene_sel1)\n",
    "    \n",
    "#     if len(gene_sel1) == 0:\n",
    "#         print(\"Warning: No genes selected for fitting models.\")\n",
    "#         return None\n",
    "    \n",
    "#     results = Parallel(n_jobs=n_jobs)(\n",
    "#         delayed(fit_with_simulated_params)(\n",
    "#             x[:, i], \n",
    "#             simulated_means[gene_names[i]], \n",
    "#             simulated_vars[gene_names[i]],\n",
    "#             simulated_pis[gene_names[i]],\n",
    "#             maxiter\n",
    "#         )\n",
    "#         for i in tqdm(gene_sel1, desc=\"Fitting models\")\n",
    "#     )\n",
    "    \n",
    "#     params_df = pd.DataFrame(results, \n",
    "#                            index=[gene_names[i] for i in gene_sel1], \n",
    "#                            columns=['pi0', 'theta', 'mu', 'model_selected'])\n",
    "    \n",
    "#     model_params = {\n",
    "#         'gene_sel1': {i: gene_names[i] for i in gene_sel1},\n",
    "#         'gene_sel2': {i: gene_names[i] for i in gene_sel2},\n",
    "#         'marginal_param1': params_df[['pi0', 'theta', 'mu']].values.tolist(),\n",
    "#         'model_selected': params_df['model_selected'].tolist(),\n",
    "#         'min_nonzero_num': min_nonzero_num,\n",
    "#         'n_cell': n,\n",
    "#         'n_read': np.sum(x)\n",
    "#     }\n",
    "    \n",
    "#     return model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simulate_gene(iter, gene_names, adata, model_params, rr):\n",
    "    gene_name = gene_names[iter]\n",
    "    if gene_name in adata.var_names:\n",
    "        gene_expr = adata[:, gene_name].X.toarray().flatten()\n",
    "\n",
    "        print(f\"\\nSimulating gene: {gene_name}\")\n",
    "        print(f\"Original stats - Min: {gene_expr.min():.2f}, Max: {gene_expr.max():.2f}, Mean: {gene_expr.mean():.2f}\")\n",
    "\n",
    "        original_order = np.argsort(gene_expr)\n",
    "        param = model_params['marginal_param1'][iter]\n",
    "        model_type = model_params['model_selected'][iter]\n",
    "\n",
    "        param = [float(p) if p != 'inf' else np.inf for p in param]\n",
    "\n",
    "        try:\n",
    "            if model_type == 'Poisson':\n",
    "                lambda_param = param[2] * rr\n",
    "                sim_raw_expr = poisson.rvs(lambda_param, size=adata.shape[0])\n",
    "            elif model_type == 'NB':\n",
    "                r_param = param[1]\n",
    "                if np.isinf(r_param):\n",
    "                    lambda_param = param[2] * rr\n",
    "                    sim_raw_expr = poisson.rvs(lambda_param, size=adata.shape[0])\n",
    "                else:\n",
    "                    p_param = r_param / (r_param + param[2] * rr)\n",
    "                    r_param = np.maximum(r_param, 1e-8)\n",
    "                    p_param = np.clip(p_param, 1e-8, 1 - 1e-8)\n",
    "                    sim_raw_expr = nbinom.rvs(r_param, p_param, size=adata.shape[0])\n",
    "            elif model_type == 'ZIP':\n",
    "                pi0 = param[0]\n",
    "                lambda_param = param[2] * rr\n",
    "                zero_mask = bernoulli.rvs(pi0, size=adata.shape[0])\n",
    "                sim_raw_expr = poisson.rvs(lambda_param, size=adata.shape[0]) * (1 - zero_mask)\n",
    "            elif model_type == 'ZINB':\n",
    "                pi0 = param[0]\n",
    "                r_param = param[1]\n",
    "                p_param = r_param / (r_param + param[2] * rr)\n",
    "\n",
    "                if r_param <= 0 or not (0 < p_param < 1):\n",
    "                    raise ValueError(f\"Invalid parameters for nbinom: r = {r_param}, p = {p_param}\")\n",
    "                \n",
    "                zero_mask = bernoulli.rvs(pi0, size=adata.shape[0])\n",
    "                sim_raw_expr = nbinom.rvs(r_param, p_param, size=adata.shape[0]) * (1 - zero_mask)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error simulating gene {gene_name} with {model_type} model: {e}\")\n",
    "            print(\"Falling back to Poisson distribution with mean as lambda\")\n",
    "            # 使用原始数据的平均值作为 Poisson 分布的参数\n",
    "            mean_expr = np.mean(gene_expr)\n",
    "            sim_raw_expr = poisson.rvs(mean_expr, size=adata.shape[0])\n",
    "        \n",
    "        try:\n",
    "            sim_order = np.argsort(sim_raw_expr)\n",
    "            final_expr = np.zeros_like(gene_expr)\n",
    "            final_expr[original_order] = sim_raw_expr[sim_order]\n",
    "\n",
    "            top_10_percent = np.percentile(gene_expr, 90)\n",
    "            original_high = gene_expr > top_10_percent\n",
    "            final_high = final_expr > top_10_percent\n",
    "            overlap = np.sum(original_high & final_high) / np.sum(original_high)\n",
    "            print(f\"Top 10% preservation: {overlap * 100:.2f}%\")\n",
    "\n",
    "            correlation = np.corrcoef(gene_expr, final_expr)[0, 1]\n",
    "            print(f\"Correlation: {correlation:.4f}\")\n",
    "\n",
    "            return final_expr\n",
    "        except Exception as e:\n",
    "            print(f\"Error in post-processing for gene {gene_name}: {e}\")\n",
    "            # 如果后处理也失败，直接返回 Poisson 模拟结果\n",
    "            return sim_raw_expr\n",
    "    else:\n",
    "        return np.zeros(adata.shape[0])\n",
    "\n",
    "def srtsim_count_single(model_params, adata, rr=1, breaktie='random', num_cores=1):\n",
    "    p1 = len(model_params['gene_sel1'])\n",
    "    p2 = len(model_params['gene_sel2'])\n",
    "    gene_names = [None] * (p1 + p2)\n",
    "\n",
    "    for idx, (i, gene_name) in enumerate(model_params['gene_sel1'].items()):\n",
    "        if idx < p1:\n",
    "            gene_names[idx] = gene_name\n",
    "        else:\n",
    "            raise IndexError(f\"Index {idx} out of range for gene_sel1\")\n",
    "\n",
    "    for idx, (i, gene_name) in enumerate(model_params['gene_sel2'].items()):\n",
    "        if idx + p1 < p1 + p2:\n",
    "            gene_names[idx + p1] = gene_name\n",
    "        else:\n",
    "            raise IndexError(f\"Index {idx + p1} out of range for gene_sel2\")\n",
    "\n",
    "    if p2 > 0:\n",
    "        adata = adata[:, ~adata.var_names.isin(model_params['gene_sel2'])]\n",
    "\n",
    "    num_loc, num_genes = adata.shape\n",
    "    result = np.zeros((p1 + p2, num_loc), dtype=float)\n",
    "\n",
    "    if p1 > 0:\n",
    "        for iter in range(p1):\n",
    "            result[iter, :] = simulate_gene(iter, gene_names, adata, model_params, rr)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def srtsim_remain_simulate_count(simsrt, adata, breaktie='random', rrr=None, num_cores=8, verbose=False):\n",
    "    if simsrt.simcolData is None:\n",
    "        simsrt.simcolData = simsrt.refcolData.copy()\n",
    "\n",
    "    oldnum_loc = simsrt.refcolData.shape[0]\n",
    "    newnum_loc = simsrt.simcolData.shape[0]\n",
    "    param_res = simsrt.EstParam[0] \n",
    "\n",
    "    # Calculate total counts in the old data\n",
    "    total_count_old = adata.obs['total_counts'].sum()\n",
    "    total_count_new = total_count_old  \n",
    "    r = (total_count_new / newnum_loc) / (total_count_old / oldnum_loc) if rrr is None else rrr\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"The ratio between the seqdepth per location is: {r}\")\n",
    "\n",
    "    # Generate the simulated count matrix\n",
    "    rawcount = srtsim_count_single(model_params=param_res, adata=adata, rr=r, breaktie=breaktie, num_cores=num_cores)\n",
    "    \n",
    "\n",
    "    # Handle all-zero genes\n",
    "    all_zero_idx = np.where(np.sum(rawcount, axis=1) == 0)[0]\n",
    "    if len(all_zero_idx) > 0:\n",
    "        for idx in all_zero_idx:\n",
    "            nonzero_idx = np.random.choice(newnum_loc, 1)[0]\n",
    "            rawcount[idx, nonzero_idx] = 1\n",
    "\n",
    "    outcount = np.round(rawcount).astype(int)\n",
    "    simsrt.simCounts = sp.csr_matrix(outcount)\n",
    "\n",
    "    return simsrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import numpy as np\n",
    "\n",
    "def run_simulation_tissue(adata, mode=\"strict\", threshold=0.99):\n",
    "    adata.var_names_make_unique()\n",
    "    adata.obs_names_make_unique()\n",
    "    sc.pp.calculate_qc_metrics(adata, inplace=True)\n",
    "\n",
    "    all_genes = adata.var_names.tolist()\n",
    "    print(adata)\n",
    "\n",
    "    # 模拟均值\n",
    "    # simulated_means, mean_threshold, mean_evaluation = simulate_gene_means(adata, mode=mode, threshold=threshold)\n",
    "    # print(f\"Mean simulation - Best threshold: {mean_threshold}\")\n",
    "    # print(f\"Mean simulation - Evaluation: {mean_evaluation}\")\n",
    "\n",
    "    simulated_means = simulate_gene_average_expression(adata)\n",
    "    # 模拟方差\n",
    "    simulated_vars, var_evaluation= simulate_gene_variances_advanced(adata)\n",
    "    \n",
    "\n",
    "    print(f\"Variance simulation - Evaluation: {var_evaluation}\")\n",
    "\n",
    "    print(f\"Number of simulated genes: {len(simulated_means)}\")\n",
    "    # 使用模拟的参数拟合边际模型\n",
    "    model_params = fit_marginal_model_with_simulated_params(\n",
    "        adata, \n",
    "        simulated_means, \n",
    "        simulated_vars, \n",
    "        min_nonzero_num=2, \n",
    "        maxiter=500, \n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # 添加模拟评估结果到返回的字典中\n",
    "    model_params['simulation_evaluation'] = {\n",
    "        'variance': var_evaluation,\n",
    "\n",
    "    }\n",
    "\n",
    "    return model_params\n",
    "\n",
    "\n",
    "\n",
    "class SimSRT:\n",
    "    def __init__(self, adata, model_params):\n",
    "        self.refCounts = adata.to_df()  \n",
    "        self.refcolData = adata.obs.copy()  \n",
    "        self.simcolData = None\n",
    "        self.EstParam = [model_params]\n",
    "        self.simCounts = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read_h5ad(\"/Users/chen_yiru/Desktop/simulation/data/raw/Sample_data_151676.h5ad\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tissue-base 最基础的模拟测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 2701 × 10778\n",
      "    obs: 'in_tissue', 'array_row', 'array_col', 'imagerow', 'imagecol', 'sum_umi', 'sum_gene', 'subject', 'position', 'replicate', 'discard', 'cell_count', 'layer_guess', 'layer_guess_reordered', 'layer_guess_reordered_short', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'pct_counts_in_top_200_genes', 'pct_counts_in_top_500_genes', 'n_counts'\n",
      "    var: 'gene_ids', 'feature_types', 'genome', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts', 'n_counts'\n",
      "    uns: 'layer_guess_reordered_colors'\n",
      "    obsm: 'rgb', 'spatial'\n",
      "\n",
      "Final evaluation:\n",
      "{\"Cohen's d\": -0.0006861767930373992, 'Relative Error': 0.019799040027003077, 'KS Statistic': 0.029504546298014467, 'Correlation': 0.9762400072371336, 'Quantile Relative Errors': {'25.0th': 0.05552662590941831, '50.0th': 0.00561315079130267, '75.0th': 0.00611720610413049, '90.0th': 0.03674863891475361, '95.0th': 0.06765484920494956, '99.0th': 0.35145308387363067, '100th': 0.0}, 'Verdict': 'Excellent fit'}\n",
      "\n",
      "Diagnostic Information:\n",
      "Original data mean: 1.8255774974822998\n",
      "Original data variance: 2714.269775390625\n",
      "Simulated data mean: 1.861722179427348\n",
      "Simulated data variance: 2834.6245640281454\n",
      "Best threshold: 99.5\n",
      "Best alpha: 1.21766933036688\n",
      "Best beta: 0.12424778618495445\n",
      "Variance simulation - Best threshold: 99.5\n",
      "Variance simulation - Evaluation: {\"Cohen's d\": -0.0006861767930373992, 'Relative Error': 0.019799040027003077, 'KS Statistic': 0.029504546298014467, 'Correlation': 0.9762400072371336, 'Quantile Relative Errors': {'25.0th': 0.05552662590941831, '50.0th': 0.00561315079130267, '75.0th': 0.00611720610413049, '90.0th': 0.03674863891475361, '95.0th': 0.06765484920494956, '99.0th': 0.35145308387363067, '100th': 0.0}, 'Verdict': 'Excellent fit'}\n",
      "Number of simulated genes: 10778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_params \u001b[38;5;241m=\u001b[39m \u001b[43mrun_simulation_tissue\u001b[49m\u001b[43m(\u001b[49m\u001b[43madata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m simsrt \u001b[38;5;241m=\u001b[39m SimSRT(adata, model_params)\n\u001b[1;32m      4\u001b[0m simulated_simsrt \u001b[38;5;241m=\u001b[39m srtsim_remain_simulate_count(simsrt, adata, num_cores\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[59], line 26\u001b[0m, in \u001b[0;36mrun_simulation_tissue\u001b[0;34m(adata, mode, threshold)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of simulated genes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(simulated_means)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# 使用模拟的参数拟合边际模型\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m model_params \u001b[38;5;241m=\u001b[39m \u001b[43mfit_marginal_model_with_simulated_params\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43madata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43msimulated_means\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43msimulated_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_nonzero_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     33\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# 添加模拟评估结果到返回的字典中\u001b[39;00m\n\u001b[1;32m     36\u001b[0m model_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimulation_evaluation\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariance\u001b[39m\u001b[38;5;124m'\u001b[39m: var_evaluation,\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m }\n",
      "Cell \u001b[0;32mIn[56], line 92\u001b[0m, in \u001b[0;36mfit_marginal_model_with_simulated_params\u001b[0;34m(adata, simulated_means, simulated_vars, min_nonzero_num, maxiter, n_jobs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: No genes selected for fitting models.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfit_with_simulated_mean_and_var\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43msimulated_means\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgene_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43msimulated_vars\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgene_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaxiter\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgene_sel1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFitting models\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m params_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results, \n\u001b[1;32m    103\u001b[0m                        index\u001b[38;5;241m=\u001b[39m[gene_names[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m gene_sel1], \n\u001b[1;32m    104\u001b[0m                        columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpi0\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtheta\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmu\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_selected\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    106\u001b[0m model_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgene_sel1\u001b[39m\u001b[38;5;124m'\u001b[39m: {i: gene_names[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m gene_sel1},\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgene_sel2\u001b[39m\u001b[38;5;124m'\u001b[39m: {i: gene_names[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m gene_sel2},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_read\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39msum(x)\n\u001b[1;32m    114\u001b[0m }\n",
      "File \u001b[0;32m~/miniconda3/envs/simulation/lib/python3.9/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/simulation/lib/python3.9/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/simulation/lib/python3.9/site-packages/joblib/parallel.py:1768\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n\u001b[0;32m-> 1768\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   1769\u001b[0m     batched_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[1;32m   1771\u001b[0m \u001b[38;5;66;03m# Flatten the batched results to output one output at a time\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    }
   ],
   "source": [
    "model_params = run_simulation_tissue(adata)\n",
    "simsrt = SimSRT(adata, model_params)\n",
    "\n",
    "simulated_simsrt = srtsim_remain_simulate_count(simsrt, adata, num_cores=8, verbose=True)\n",
    "\n",
    "simulated_counts = simulated_simsrt.simCounts\n",
    "\n",
    "if simulated_counts.shape != adata.shape:\n",
    "    print(f\"Warning: simulated_counts shape {simulated_counts.shape} does not match adata shape {adata.shape}\")\n",
    "    if simulated_counts.shape == (adata.shape[1], adata.shape[0]):\n",
    "        simulated_counts = simulated_counts.T  \n",
    "    elif simulated_counts.shape != adata.shape:\n",
    "        raise ValueError(\"Cannot adjust simulated_counts shape to match adata shape\")\n",
    "\n",
    "simulated_adata = anndata.AnnData(\n",
    "    X=simulated_counts,\n",
    "    obs=adata.obs.copy(),\n",
    "    var=adata.var.copy(),\n",
    "    obsm={'spatial': adata.obsm['spatial']}\n",
    ")\n",
    "\n",
    "simulated_adata.obs['total_counts'] = simulated_adata.X.sum(axis=1)\n",
    "simulated_adata.obs['n_genes'] = (simulated_adata.X > 0).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_adata.write_h5ad(\"/Users/chen_yiru/Desktop/simulation/data/simulated/simulated_Sample_data_151673.h5ad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## domain-based 模拟测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse as sp\n",
    "from scipy.stats import genpareto, ks_2samp\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import gammaln\n",
    "\n",
    "class GPD_VarianceSimulator:\n",
    "    def __init__(self):\n",
    "        self.c = None  # shape parameter\n",
    "        self.loc = None  # location parameter\n",
    "        self.scale = None  # scale parameter\n",
    "        self.threshold = None\n",
    "        self.tail_data = None\n",
    "        self.original_order = None\n",
    "        self.original_data = None\n",
    "        self.random_state = None\n",
    "\n",
    "    def extract_data(self, X):\n",
    "        variances = np.var(X, axis=0, ddof=1)\n",
    "        variances = np.nan_to_num(variances, nan=np.nanmean(variances))\n",
    "        variances = np.maximum(variances, 1e-10)\n",
    "        return variances\n",
    "\n",
    "    def assess_tail_discreteness(self, tail_data):\n",
    "        sorted_tail = np.sort(tail_data)\n",
    "        differences = np.diff(sorted_tail)\n",
    "        cv = np.std(differences) / np.mean(differences)\n",
    "\n",
    "        if cv > 2.0:\n",
    "            return 'discrete'\n",
    "        elif cv < 1.0:\n",
    "            return 'smooth'\n",
    "        else:\n",
    "            return 'mixed'\n",
    "\n",
    "    def fit_single_component(self, data):\n",
    "        try:\n",
    "            params = genpareto.fit(data)\n",
    "            return params[0], params[1], params[2]  # c, loc, scale\n",
    "        except Exception as e:\n",
    "            print(f\"Single component fitting failed: {str(e)}\")\n",
    "            return 0.1, np.min(data), np.std(data)  # Fallback values\n",
    "\n",
    "    def fit(self, adata):\n",
    "        try:\n",
    "            if sp.issparse(adata.X):\n",
    "                X = adata.X.toarray()\n",
    "            else:\n",
    "                X = adata.X\n",
    "\n",
    "            self.original_data = self.extract_data(X)\n",
    "            if not np.all(np.isfinite(self.original_data)):\n",
    "                raise ValueError(\"Data contains non-finite values\")\n",
    "\n",
    "            self.original_order = np.argsort(self.original_data)\n",
    "            sorted_data = np.sort(self.original_data)\n",
    "\n",
    "            thresholds = [99.5]\n",
    "            best_score = float('inf')\n",
    "            best_evaluation = None\n",
    "            for percentile in thresholds:\n",
    "                current_threshold = np.percentile(sorted_data, percentile)\n",
    "                main_data = sorted_data[sorted_data <= current_threshold]\n",
    "                tail_data = sorted_data[sorted_data > current_threshold]\n",
    "\n",
    "                if len(main_data) == 0 or len(tail_data) == 0:\n",
    "                    print(f\"Warning: Empty main_data or tail_data at threshold {percentile}\")\n",
    "                    continue\n",
    "\n",
    "                c, loc, scale = self.fit_single_component(main_data)\n",
    "\n",
    "                n_main = len(main_data)\n",
    "                n_tail = len(tail_data)\n",
    "\n",
    "                new_main = genpareto.rvs(c, loc=loc, scale=scale, size=n_main, random_state=self.random_state)\n",
    "\n",
    "                tail_type = self.assess_tail_discreteness(tail_data)\n",
    "                if tail_type == 'discrete':\n",
    "                    new_tail = self.random_state.choice(tail_data, size=n_tail, replace=True)\n",
    "                else:\n",
    "                    new_tail = np.interp(\n",
    "                        np.linspace(0, 1, n_tail),\n",
    "                        np.linspace(0, 1, len(tail_data)),\n",
    "                        np.sort(tail_data)\n",
    "                    )\n",
    "\n",
    "                new_samples = np.concatenate([new_main, new_tail])\n",
    "                new_samples = np.clip(new_samples, np.min(self.original_data), np.max(self.original_data))\n",
    "\n",
    "                evaluation = self.evaluate_fit(self.original_data, new_samples)\n",
    "\n",
    "                score = (abs(evaluation[\"Cohen's d\"]) + \n",
    "                         evaluation[\"Relative Error\"] + \n",
    "                         evaluation[\"KS Statistic\"] + \n",
    "                         (1 - evaluation[\"Correlation\"]))\n",
    "\n",
    "                if score < best_score:\n",
    "                    best_score = score\n",
    "                    self.threshold = percentile\n",
    "                    self.c, self.loc, self.scale = c, loc, scale\n",
    "                    best_evaluation = evaluation\n",
    "                    self.tail_data = tail_data\n",
    "\n",
    "            if self.threshold is None:\n",
    "                print(\"Warning: No valid threshold found, fallback to default.\")\n",
    "                self.threshold = 95  # Default threshold\n",
    "\n",
    "            return self, best_evaluation\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Fitting failed: {str(e)}\")\n",
    "            self.c, self.loc, self.scale = self.fit_single_component(self.original_data)\n",
    "            return self, {\"Verdict\": \"Fallback to single component\"}\n",
    "\n",
    "    def simulate(self, n_samples):\n",
    "        try:\n",
    "            if self.threshold is None:\n",
    "                print(\"Warning: Threshold is None, using default value of 95.\")\n",
    "                self.threshold = 95\n",
    "\n",
    "            n_main = int(n_samples * self.threshold / 100)\n",
    "            n_tail = n_samples - n_main\n",
    "\n",
    "            new_main = genpareto.rvs(self.c, loc=self.loc, scale=self.scale, size=n_main, random_state=self.random_state)\n",
    "\n",
    "            tail_type = self.assess_tail_discreteness(self.tail_data)\n",
    "            if tail_type == 'discrete':\n",
    "                new_tail = self.random_state.choice(self.tail_data, size=n_tail, replace=True)\n",
    "            else:\n",
    "                new_tail = np.interp(\n",
    "                    np.linspace(0, 1, n_tail),\n",
    "                    np.linspace(0, 1, len(self.tail_data)),\n",
    "                    np.sort(self.tail_data)\n",
    "                )\n",
    "\n",
    "            new_samples = np.concatenate([new_main, new_tail])\n",
    "            new_samples = np.clip(new_samples, np.min(self.original_data), np.max(self.original_data))\n",
    "            new_samples = np.sort(new_samples)\n",
    "\n",
    "            simulated_data = np.zeros_like(new_samples)\n",
    "            simulated_data[self.original_order] = new_samples\n",
    "\n",
    "            return simulated_data\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Simulation failed: {str(e)}\")\n",
    "            return self.random_state.choice(self.original_data, size=n_samples, replace=True)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate_fit(original, generated, quantiles=[0.25, 0.5, 0.75, 0.9, 0.95, 0.99, 1]):\n",
    "        \"\"\"评估生成的数据与原始数据的拟合度\"\"\"\n",
    "        def cohens_d(x1, x2):\n",
    "            n1, n2 = len(x1), len(x2)\n",
    "            var1, var2 = np.var(x1, ddof=1), np.var(x2, ddof=1)\n",
    "            pooled_se = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))\n",
    "            return (np.mean(x1) - np.mean(x2)) / pooled_se\n",
    "\n",
    "        def relative_error(x1, x2):\n",
    "            return np.abs(np.mean(x1) - np.mean(x2)) / np.mean(x1)\n",
    "\n",
    "        effect_size = cohens_d(original, generated)\n",
    "        rel_error = relative_error(original, generated)\n",
    "        ks_stat, _ = ks_2samp(original, generated)\n",
    "        correlation = np.corrcoef(np.sort(original), np.sort(generated))[0, 1]\n",
    "\n",
    "        orig_quant = np.quantile(original, quantiles)\n",
    "        gen_quant = np.quantile(generated, quantiles)\n",
    "        quant_rel_errors = np.abs(orig_quant - gen_quant) / orig_quant\n",
    "\n",
    "        results = {\n",
    "            \"Cohen's d\": effect_size,\n",
    "            \"Relative Error\": rel_error,\n",
    "            \"KS Statistic\": ks_stat,\n",
    "            \"Correlation\": correlation,\n",
    "            \"Quantile Relative Errors\": dict(zip([f\"{q*100}th\" for q in quantiles], quant_rel_errors))\n",
    "        }\n",
    "\n",
    "        excellent = (abs(effect_size) < 0.05 and rel_error < 0.05 and ks_stat < 0.1 and correlation > 0.95)\n",
    "        good = (abs(effect_size) < 0.1 and rel_error < 0.15 and ks_stat < 0.15 and correlation > 0.9)\n",
    "        fair = (abs(effect_size) < 0.2 and rel_error < 0.2 and ks_stat < 0.2 and correlation > 0.8)\n",
    "\n",
    "        if excellent:\n",
    "            verdict = \"Excellent fit\"\n",
    "        elif good:\n",
    "            verdict = \"Good fit\"\n",
    "        elif fair:\n",
    "            verdict = \"Fair fit\"\n",
    "        else:\n",
    "            verdict = \"Poor fit\"\n",
    "\n",
    "        results[\"Verdict\"] = verdict\n",
    "\n",
    "        return results\n",
    "\n",
    "    def fit_and_simulate(self, adata, n_iterations=30):\n",
    "        best_simulation = None\n",
    "        best_evaluation = None\n",
    "        best_score = float('inf')\n",
    "        best_threshold = None\n",
    "        best_c = None\n",
    "        best_loc = None\n",
    "        best_scale = None\n",
    "\n",
    "        for _ in range(n_iterations):\n",
    "            self.random_state = np.random.RandomState()  # 每次迭代使用新的随机种子\n",
    "            self, evaluation = self.fit(adata)\n",
    "            simulated_values = self.simulate(adata.n_vars)\n",
    "            final_evaluation = self.evaluate_fit(self.original_data, simulated_values)\n",
    "\n",
    "            score = (abs(final_evaluation[\"Cohen's d\"]) + \n",
    "                    final_evaluation[\"Relative Error\"] + \n",
    "                    final_evaluation[\"KS Statistic\"] + \n",
    "                    (1 - final_evaluation[\"Correlation\"]))\n",
    "\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_simulation = simulated_values\n",
    "                best_evaluation = final_evaluation\n",
    "                best_threshold = self.threshold\n",
    "                best_c = self.c\n",
    "                best_loc = self.loc\n",
    "                best_scale = self.scale\n",
    "\n",
    "        self.threshold = best_threshold\n",
    "        self.c = best_c\n",
    "        self.loc = best_loc\n",
    "        self.scale = best_scale\n",
    "\n",
    "        return best_simulation, best_evaluation\n",
    "\n",
    "\n",
    "def simulate_gene_variances_domain(adata, n_iterations=10):\n",
    "    simulator = GPD_VarianceSimulator()\n",
    "    simulated_values, final_evaluation = simulator.fit_and_simulate(adata, n_iterations)\n",
    "\n",
    "    result_dict = dict(zip(adata.var_names, simulated_values))\n",
    "    return result_dict, simulator.threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import scanpy as sc\n",
    "from scipy.stats import invgamma, ks_2samp\n",
    "from scipy.special import logsumexp\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "class IG_VarianceSimulator:\n",
    "    def __init__(self):\n",
    "        self.components = []\n",
    "        self.weights = None\n",
    "        self.alphas = None\n",
    "        self.betas = None\n",
    "        self.threshold = None\n",
    "        self.tail_data = None\n",
    "        self.original_order = None\n",
    "        self.original_data = None\n",
    "        self.random_state = None\n",
    "        self.n_components = None\n",
    "\n",
    "    def fit_mixture_components(self, data, max_components=10, max_iter=50):\n",
    "        \"\"\"使用EM算法拟合混合逆伽马分布\"\"\"\n",
    "        best_bic = float('inf')\n",
    "        best_params = None\n",
    "        best_n_components = None\n",
    "        \n",
    "        # 确保数据是有效的\n",
    "        data = np.array(data)\n",
    "        data = data[data > 0]  # 确保所有值都是正数\n",
    "        \n",
    "        # 预计算数据统计量\n",
    "        data_mean = np.mean(data)\n",
    "        data_std = np.std(data)\n",
    "        \n",
    "        # 从最简单的模型开始\n",
    "        for n_components in range(1, max_components + 1):\n",
    "            try:\n",
    "                # 更保守的初始化\n",
    "                weights = np.ones(n_components) / n_components\n",
    "                alphas = np.ones(n_components) * 2  # 从更安全的值开始\n",
    "                betas = np.ones(n_components) * data_mean\n",
    "                \n",
    "                prev_ll = -np.inf\n",
    "                for iter_count in range(max_iter):\n",
    "                    # E步：计算责任\n",
    "                    resp = np.zeros((len(data), n_components))\n",
    "                    for j in range(n_components):\n",
    "                        resp[:, j] = weights[j] * invgamma.pdf(data, alphas[j], scale=betas[j])\n",
    "                    resp_sum = resp.sum(axis=1, keepdims=True)\n",
    "                    resp_sum[resp_sum == 0] = 1e-300  # 避免除以零\n",
    "                    resp /= resp_sum\n",
    "                    \n",
    "                    # M步：更新参数\n",
    "                    weights = resp.mean(axis=0)\n",
    "                    weights /= weights.sum()  # 确保权重和为1\n",
    "                    \n",
    "                    for j in range(n_components):\n",
    "                        try:\n",
    "                            mask = resp[:, j] > 0.01  # 降低阈值\n",
    "                            if np.sum(mask) > 3:  # 确保有足够的数据点\n",
    "                                component_data = data[mask]\n",
    "                                # 使用更稳定的参数估计\n",
    "                                params = invgamma.fit(component_data, floc=0)\n",
    "                                alphas[j] = max(1.1, min(10, params[0]))  # 限制alpha的范围\n",
    "                                betas[j] = max(data_mean/10, min(data_mean*10, params[2]))  # 限制beta的范围\n",
    "                        except:\n",
    "                            continue\n",
    "                    \n",
    "                    # 计算当前似然\n",
    "                    current_ll = np.sum(np.log(np.sum(weights[None, :] * \n",
    "                                    invgamma.pdf(data[:, None], alphas[None, :], scale=betas[None, :]), axis=1)))\n",
    "                    \n",
    "                    # 检查收敛\n",
    "                    if abs(current_ll - prev_ll) < 1e-3 * abs(prev_ll):\n",
    "                        break\n",
    "                    prev_ll = current_ll\n",
    "                \n",
    "                # 计算BIC\n",
    "                bic = -2 * current_ll + np.log(len(data)) * (3 * n_components - 1)\n",
    "                \n",
    "                if bic < best_bic:\n",
    "                    best_bic = bic\n",
    "                    best_params = (weights, alphas, betas)\n",
    "                    best_n_components = n_components\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Component {n_components} fitting failed: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if best_params is None:\n",
    "            # 如果所有拟合都失败，返回简单的单组件拟合\n",
    "            return (np.array([1.0]), np.array([2.0]), np.array([data_mean])), 1\n",
    "        \n",
    "        return best_params, best_n_components\n",
    "\n",
    "    def fit(self, adata):\n",
    "        try:\n",
    "            if sp.issparse(adata.X):\n",
    "                X = adata.X.toarray()\n",
    "            else:\n",
    "                X = adata.X\n",
    "\n",
    "            self.original_data = self.extract_data(X)\n",
    "            if not np.all(np.isfinite(self.original_data)):\n",
    "                raise ValueError(\"Data contains non-finite values\")\n",
    "\n",
    "            # 确保数据是有效的\n",
    "            if len(self.original_data) == 0:\n",
    "                raise ValueError(\"No valid data after preprocessing\")\n",
    "\n",
    "            self.original_order = np.argsort(self.original_data)\n",
    "            sorted_data = np.sort(self.original_data)\n",
    "\n",
    "            # 尝试更多的阈值\n",
    "            thresholds = [95, 97.5, 99, 99.5]\n",
    "            best_score = float('inf')\n",
    "            best_params = None\n",
    "            best_evaluation = None\n",
    "            \n",
    "            for threshold in thresholds:\n",
    "                threshold_value = np.percentile(sorted_data, threshold)\n",
    "                main_data = sorted_data[sorted_data <= threshold_value]\n",
    "                tail_data = sorted_data[sorted_data > threshold_value]\n",
    "\n",
    "                if len(main_data) < 10 or len(tail_data) < 5:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    (weights, alphas, betas), n_components = self.fit_mixture_components(main_data)\n",
    "                    \n",
    "                    self.weights, self.alphas, self.betas = weights, alphas, betas\n",
    "                    self.n_components = n_components\n",
    "                    self.threshold = threshold\n",
    "                    self.tail_data = tail_data\n",
    "                    \n",
    "                    simulated = self.simulate(len(self.original_data))\n",
    "                    evaluation = self.evaluate_fit(self.original_data, simulated)\n",
    "                    \n",
    "                    score = (abs(evaluation[\"Cohen's d\"]) + \n",
    "                            evaluation[\"Relative Error\"] + \n",
    "                            evaluation[\"KS Statistic\"] + \n",
    "                            (1 - evaluation[\"Correlation\"]))\n",
    "                    \n",
    "                    if score < best_score:\n",
    "                        best_score = score\n",
    "                        best_params = (weights, alphas, betas, n_components)\n",
    "                        best_evaluation = evaluation\n",
    "                        self.tail_data = tail_data\n",
    "                        self.threshold = threshold\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Threshold {threshold} fitting failed: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "            if best_params is None:\n",
    "                # 如果所有拟合都失败，使用简单的单组件拟合\n",
    "                self.weights = np.array([1.0])\n",
    "                self.alphas = np.array([2.0])\n",
    "                self.betas = np.array([np.mean(self.original_data)])\n",
    "                self.n_components = 1\n",
    "                self.threshold = 99\n",
    "                self.tail_data = sorted_data[sorted_data > np.percentile(sorted_data, 99)]\n",
    "                return self, {\"Verdict\": \"Fallback to simple fit\"}\n",
    "\n",
    "            self.weights, self.alphas, self.betas, self.n_components = best_params\n",
    "            print(self.weights, self.alphas, self.betas, self.n_components)\n",
    "            return self, best_evaluation\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Fitting failed: {str(e)}\")\n",
    "            return self, {\"Verdict\": \"Fitting failed\"}\n",
    "    def extract_data(self, X):\n",
    "        try:\n",
    "            # 计算方差\n",
    "            variances = np.var(X, axis=0, ddof=1)\n",
    "            \n",
    "            # 如果是稀疏矩阵，转换为数组\n",
    "            if sp.issparse(variances):\n",
    "                variances = variances.A1\n",
    "                \n",
    "            # 处理可能的nan值\n",
    "            variances = np.nan_to_num(variances, nan=np.nanmean(variances))\n",
    "            \n",
    "            # 确保所有方差都是正数且不为零\n",
    "            variances = np.maximum(variances, 1e-10)\n",
    "            \n",
    "            return variances\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in extract_data: {str(e)}\")\n",
    "            return None\n",
    "        \n",
    "    def assess_tail_discreteness(self, tail_data):\n",
    "    \n",
    "        try:\n",
    "            # 对数据进行排序\n",
    "            sorted_tail = np.sort(tail_data)\n",
    "            \n",
    "            # 计算相邻值之间的差异\n",
    "            differences = np.diff(sorted_tail)\n",
    "            \n",
    "            # 计算变异系数 (CV = 标准差/平均值)\n",
    "            if np.mean(differences) == 0:\n",
    "                return 'discrete'\n",
    "                \n",
    "            cv = np.std(differences) / np.mean(differences)\n",
    "            \n",
    "            # 根据变异系数判断离散程度\n",
    "            if cv > 2.0:\n",
    "                return 'discrete'  # 高变异表示离散\n",
    "            elif cv < 1.0:\n",
    "                return 'smooth'   # 低变异表示平滑\n",
    "            else:\n",
    "                return 'mixed'    # 介于之间表示混合\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in assess_tail_discreteness: {str(e)}\")\n",
    "            return 'discrete'  # 发生错误时默认返回离散\n",
    "    def fit_and_simulate(self, adata, n_iterations=10):  # 减少迭代次数\n",
    "        \"\"\"多次拟合和模拟，选择最佳结果\"\"\"\n",
    "        best_simulation = None\n",
    "        best_evaluation = None\n",
    "        best_score = float('inf')\n",
    "        best_params = None\n",
    "\n",
    "        for _ in range(n_iterations):\n",
    "            self.random_state = np.random.RandomState()\n",
    "            try:\n",
    "                self, evaluation = self.fit(adata)\n",
    "                if evaluation.get(\"Verdict\") == \"Fitting failed\":\n",
    "                    continue\n",
    "                    \n",
    "                simulated_values = self.simulate(len(self.original_data))\n",
    "                final_evaluation = self.evaluate_fit(self.original_data, simulated_values)\n",
    "\n",
    "                score = (abs(final_evaluation[\"Cohen's d\"]) + \n",
    "                        final_evaluation[\"Relative Error\"] + \n",
    "                        final_evaluation[\"KS Statistic\"] + \n",
    "                        (1 - final_evaluation[\"Correlation\"]))\n",
    "\n",
    "                if score < best_score:\n",
    "                    best_score = score\n",
    "                    best_simulation = simulated_values\n",
    "                    best_evaluation = final_evaluation\n",
    "                    best_params = {\n",
    "                        'weights': self.weights.copy(),\n",
    "                        'alphas': self.alphas.copy(),\n",
    "                        'betas': self.betas.copy(),\n",
    "                        'n_components': self.n_components,\n",
    "                        'threshold': self.threshold,\n",
    "                        'tail_data': self.tail_data.copy() if self.tail_data is not None else None\n",
    "                    }\n",
    "\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "        if best_params is not None:\n",
    "            self.weights = best_params['weights']\n",
    "            self.alphas = best_params['alphas']\n",
    "            self.betas = best_params['betas']\n",
    "            self.n_components = best_params['n_components']\n",
    "            self.threshold = best_params['threshold']\n",
    "            self.tail_data = best_params['tail_data']\n",
    "            return best_simulation, best_evaluation\n",
    "        else:\n",
    "            print(\"Warning: No successful fit found in any iteration\")\n",
    "            return None, {\"Verdict\": \"All iterations failed\"}\n",
    "\n",
    "    def simulate(self, n_samples):\n",
    "        \"\"\"优化后的模拟方法\"\"\"\n",
    "        try:\n",
    "            n_main = int(n_samples * self.threshold / 100)\n",
    "            n_tail = n_samples - n_main\n",
    "\n",
    "            # 一次性生成所有样本\n",
    "            main_samples = np.zeros(n_main)\n",
    "            start_idx = 0\n",
    "            for i in range(self.n_components):\n",
    "                n_comp = int(n_main * self.weights[i])\n",
    "                if i == self.n_components - 1:  # 最后一个组件\n",
    "                    n_comp = n_main - start_idx\n",
    "                main_samples[start_idx:start_idx + n_comp] = invgamma.rvs(\n",
    "                    self.alphas[i], \n",
    "                    scale=self.betas[i], \n",
    "                    size=n_comp, \n",
    "                    random_state=self.random_state\n",
    "                )\n",
    "                start_idx += n_comp\n",
    "\n",
    "            # 高效处理尾部数据\n",
    "            tail_type = self.assess_tail_discreteness(self.tail_data)\n",
    "            if tail_type == 'discrete':\n",
    "                new_tail = self.random_state.choice(self.tail_data, size=n_tail, replace=True)\n",
    "            else:\n",
    "                # 使用线性插值\n",
    "                new_tail = np.interp(\n",
    "                    np.linspace(0, 1, n_tail),\n",
    "                    np.linspace(0, 1, len(self.tail_data)),\n",
    "                    np.sort(self.tail_data)\n",
    "                )\n",
    "\n",
    "            # 合并并排序\n",
    "            new_samples = np.concatenate([main_samples, new_tail])\n",
    "            new_samples = np.clip(new_samples, np.min(self.original_data), np.max(self.original_data))\n",
    "            new_samples = np.sort(new_samples)\n",
    "\n",
    "            # 恢复原始顺序\n",
    "            simulated_data = np.zeros_like(new_samples)\n",
    "            simulated_data[self.original_order] = new_samples\n",
    "\n",
    "            return simulated_data\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Simulation failed: {str(e)}\")\n",
    "            return self.random_state.choice(self.original_data, size=n_samples, replace=True)\n",
    "        \n",
    "    @staticmethod\n",
    "    def evaluate_fit(original, generated, quantiles=[0.25, 0.5, 0.75, 0.9, 0.95, 0.99, 1]):\n",
    "        def cohens_d(x1, x2):\n",
    "            n1, n2 = len(x1), len(x2)\n",
    "            var1, var2 = np.var(x1, ddof=1), np.var(x2, ddof=1)\n",
    "            pooled_se = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))\n",
    "            return (np.mean(x1) - np.mean(x2)) / pooled_se\n",
    "\n",
    "        def relative_error(x1, x2):\n",
    "            return np.abs(np.mean(x1) - np.mean(x2)) / np.mean(x1)\n",
    "\n",
    "        effect_size = cohens_d(original, generated)\n",
    "        rel_error = relative_error(original, generated)\n",
    "        ks_stat, _ = ks_2samp(original, generated)\n",
    "        correlation = np.corrcoef(np.sort(original), np.sort(generated))[0, 1]\n",
    "\n",
    "        orig_quant = np.quantile(original, quantiles)\n",
    "        gen_quant = np.quantile(generated, quantiles)\n",
    "        quant_rel_errors = np.abs(orig_quant - gen_quant) / orig_quant\n",
    "\n",
    "        results = {\n",
    "            \"Cohen's d\": effect_size,\n",
    "            \"Relative Error\": rel_error,\n",
    "            \"KS Statistic\": ks_stat,\n",
    "            \"Correlation\": correlation,\n",
    "            \"Quantile Relative Errors\": dict(zip([f\"{q*100}th\" for q in quantiles], quant_rel_errors))\n",
    "        }\n",
    "\n",
    "        excellent = (abs(effect_size) < 0.05 and rel_error < 0.05 and ks_stat < 0.1 and correlation > 0.95)\n",
    "        good = (abs(effect_size) < 0.1 and rel_error < 0.15 and ks_stat < 0.15 and correlation > 0.9)\n",
    "        fair = (abs(effect_size) < 0.2 and rel_error < 0.2 and ks_stat < 0.2 and correlation > 0.8)\n",
    "\n",
    "        if excellent:\n",
    "            verdict = \"Excellent fit\"\n",
    "        elif good:\n",
    "            verdict = \"Good fit\"\n",
    "        elif fair:\n",
    "            verdict = \"Fair fit\"\n",
    "        else:\n",
    "            verdict = \"Poor fit\"\n",
    "\n",
    "        results[\"Verdict\"] = verdict\n",
    "\n",
    "        return results\n",
    "    \n",
    "def simulate_gene_variances_advanced(adata, n_iterations=10):\n",
    "    simulator = IG_VarianceSimulator()\n",
    "    simulated_values, final_evaluation = simulator.fit_and_simulate(adata, n_iterations)\n",
    "    \n",
    "    result_dict = dict(zip(adata.var_names, simulated_values))\n",
    "    return result_dict, final_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation_domain(adata):\n",
    "    adata.var_names_make_unique()\n",
    "    adata.obs_names_make_unique()\n",
    "    sc.pp.calculate_qc_metrics(adata, inplace=True)\n",
    "\n",
    "    all_genes = adata.var_names.tolist()\n",
    "    print(adata)\n",
    "\n",
    "    # 模拟均值\n",
    "    # simulated_means, mean_threshold, mean_evaluation = simulate_gene_means(adata, mode=mode, threshold=threshold)\n",
    "    # print(f\"Mean simulation - Best threshold: {mean_threshold}\")\n",
    "    # print(f\"Mean simulation - Evaluation: {mean_evaluation}\")\n",
    "\n",
    "    simulated_means = simulate_gene_average_expression(adata)\n",
    "    # 模拟方差\n",
    "    simulated_vars, var_evaluation= simulate_gene_variances_advanced(adata)\n",
    "    \n",
    "\n",
    "    print(f\"Variance simulation - Evaluation: {var_evaluation}\")\n",
    "\n",
    "    print(f\"Number of simulated genes: {len(simulated_means)}\")\n",
    "    # 使用模拟的参数拟合边际模型\n",
    "    model_params = fit_marginal_model_with_simulated_params(\n",
    "        adata, \n",
    "        simulated_means, \n",
    "        simulated_vars, \n",
    "        min_nonzero_num=2, \n",
    "        maxiter=500, \n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # 添加模拟评估结果到返回的字典中\n",
    "    model_params['simulation_evaluation'] = {\n",
    "        'variance': var_evaluation,\n",
    "\n",
    "    }\n",
    "\n",
    "    return model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read_h5ad(\"/Users/chen_yiru/Desktop/simulation/data/raw/processed_151673_filtered.h5ad\")\n",
    "sc.pp.filter_genes(adata, min_cells=50)\n",
    "adata.write_h5ad(\"/Users/chen_yiru/Desktop/simulation/data/raw/processed_151673_filtered.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 3553 × 13072\n",
      "    obs: 'Ground Truth', 'n_counts', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'pct_counts_in_top_200_genes', 'pct_counts_in_top_500_genes'\n",
      "    var: 'gene_ids', 'feature_types', 'genome', 'n_cells', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts'\n",
      "    obsm: 'spatial'\n",
      "[1.] [1.1] [0.06676248] 1\n",
      "[1.] [1.1] [0.0632476] 1\n",
      "[1.] [1.1] [0.06676248] 1\n",
      "[1.] [1.1] [0.0632476] 1\n",
      "[1.] [1.1] [0.06056084] 1\n",
      "[1.] [1.1] [0.06676248] 1\n",
      "[1.] [1.1] [0.05941892] 1\n",
      "[1.] [1.1] [0.06676248] 1\n",
      "[1.] [1.1] [0.06056084] 1\n",
      "[1.] [1.1] [0.06676248] 1\n",
      "Variance simulation - Evaluation: {\"Cohen's d\": -0.0005740421480767003, 'Relative Error': 0.020187290177713767, 'KS Statistic': 0.13310893512851896, 'Correlation': 0.9835400223812112, 'Quantile Relative Errors': {'25.0th': 0.1250588361494914, '50.0th': 0.31691517451955253, '75.0th': 0.3644903867193647, '90.0th': 0.35514143701686934, '95.0th': 0.3474085648665426, '99.0th': 0.12096415881160717, '100th': 0.0}, 'Verdict': 'Good fit'}\n",
      "Number of simulated genes: 13072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting models: 100%|██████████| 13072/13072 [00:15<00:00, 863.87it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 988 × 13072\n",
      "    obs: 'Ground Truth', 'n_counts', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'pct_counts_in_top_200_genes', 'pct_counts_in_top_500_genes'\n",
      "    var: 'gene_ids', 'feature_types', 'genome', 'n_cells', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts'\n",
      "    obsm: 'spatial'\n",
      "[1.] [1.1] [0.05969268] 1\n",
      "[1.] [1.1] [0.06192278] 1\n",
      "[1.] [1.1] [0.05969268] 1\n",
      "[1.] [1.1] [0.06473195] 1\n",
      "[1.] [1.1] [0.05969268] 1\n",
      "[1.] [1.1] [0.05873957] 1\n",
      "[1.] [1.1] [0.06473195] 1\n",
      "[1.] [1.1] [0.05969268] 1\n",
      "[1.] [1.1] [0.05969268] 1\n",
      "[1.] [1.1] [0.06473195] 1\n",
      "Variance simulation - Evaluation: {\"Cohen's d\": -0.00046112815880224327, 'Relative Error': 0.016005464045839562, 'KS Statistic': 0.17120563035495717, 'Correlation': 0.9820639169133482, 'Quantile Relative Errors': {'25.0th': 0.24243494403805627, '50.0th': 0.41070195024840656, '75.0th': 0.4577085431643311, '90.0th': 0.4744260351320724, '95.0th': 0.4609861851299965, '99.0th': 0.09018578480635149, '100th': 0.0}, 'Verdict': 'Fair fit'}\n",
      "Number of simulated genes: 13072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting models: 100%|██████████| 13072/13072 [00:01<00:00, 7735.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth: Layer_3\n",
      "Number of cells: 988\n",
      "Number of genes: 13072\n",
      "AnnData object with n_obs × n_vars = 243 × 13072\n",
      "    obs: 'Ground Truth', 'n_counts', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'pct_counts_in_top_200_genes', 'pct_counts_in_top_500_genes'\n",
      "    var: 'gene_ids', 'feature_types', 'genome', 'n_cells', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts'\n",
      "    obsm: 'spatial'\n",
      "[1.] [2.] [0.07826029] 1\n",
      "[1.] [2.] [0.07826029] 1\n"
     ]
    }
   ],
   "source": [
    "def combine_models(global_params, domain_params, alpha=0.3):\n",
    "    combined_params = {}\n",
    "    \n",
    "    # 拷贝通用的参数 ('gene_sel1', 'gene_sel2')\n",
    "    for key in global_params:\n",
    "        if key in ['gene_sel1', 'gene_sel2']:\n",
    "            combined_params[key] = global_params[key]\n",
    "        elif key == 'marginal_param1':  # 处理 'marginal_param1' 参数\n",
    "            global_genes = global_params['gene_sel1'].values()\n",
    "            domain_genes = domain_params['gene_sel1'].values()\n",
    "            \n",
    "            combined_marginal_param1 = []\n",
    "            for i, gene in enumerate(global_genes):\n",
    "                if gene in domain_genes:  # 如果域模型中也有这个基因\n",
    "                    domain_idx = list(domain_genes).index(gene)\n",
    "                    # 使用全局和域的参数加权平均\n",
    "                    combined_param = alpha * np.array(global_params[key][i]) + (1 - alpha) * np.array(domain_params[key][domain_idx])\n",
    "                else:  # 如果域模型没有这个基因，直接使用全局的参数\n",
    "                    combined_param = global_params[key][i]\n",
    "                \n",
    "                # 确保参数在合理的范围内\n",
    "                combined_param[0] = min(max(combined_param[0], 0), 1)  # pi 的范围是 [0, 1]\n",
    "                combined_param[1] = max(combined_param[1], 1e-8)       # r 的最小值 1e-8，防止负值\n",
    "                combined_param[2] = max(combined_param[2], 1e-8)       # mu 的最小值 1e-8\n",
    "                \n",
    "                combined_marginal_param1.append(combined_param)\n",
    "            \n",
    "            combined_params[key] = combined_marginal_param1\n",
    "        else:\n",
    "            combined_params[key] = global_params[key]\n",
    "    \n",
    "    # 如果模型类型发生冲突，优先使用 domain 模型参数，避免使用无效的全局参数\n",
    "    for i, (global_model, domain_model) in enumerate(zip(global_params['model_selected'], domain_params['model_selected'])):\n",
    "        if global_model != domain_model:\n",
    "            print(f\"Warning: Inconsistent model types for gene {i}. Using domain model {domain_model}.\")\n",
    "\n",
    "            # 以 domain 模型为准\n",
    "            combined_params['model_selected'][i] = domain_model\n",
    "\n",
    "            # 检查全局参数是否无效，如果无效就不进行修正，直接使用 domain 参数\n",
    "            global_param = global_params['marginal_param1'][i]\n",
    "            domain_param = domain_params['marginal_param1'][i]\n",
    "\n",
    "            if domain_model == 'Poisson':\n",
    "                # 修正 mu，前提是全局参数有效\n",
    "                global_mu = global_param[2]\n",
    "                if not np.isinf(global_mu) and not np.isnan(global_mu):\n",
    "                    combined_params['marginal_param1'][i][2] = alpha * global_mu + (1 - alpha) * domain_param[2]\n",
    "                else:\n",
    "                    combined_params['marginal_param1'][i][2] = domain_param[2]\n",
    "\n",
    "            elif domain_model == 'NB':\n",
    "                # 修正 r 和 mu，前提是全局参数有效\n",
    "                global_r = global_param[1]\n",
    "                global_mu = global_param[2]\n",
    "\n",
    "                if not np.isinf(global_r) and not np.isnan(global_r):\n",
    "                    combined_params['marginal_param1'][i][1] = alpha * global_r + (1 - alpha) * domain_param[1]\n",
    "                else:\n",
    "                    combined_params['marginal_param1'][i][1] = domain_param[1]\n",
    "                \n",
    "                if not np.isinf(global_mu) and not np.isnan(global_mu):\n",
    "                    combined_params['marginal_param1'][i][2] = alpha * global_mu + (1 - alpha) * domain_param[2]\n",
    "                else:\n",
    "                    combined_params['marginal_param1'][i][2] = domain_param[2]\n",
    "\n",
    "            elif domain_model == 'ZIP':\n",
    "                # 修正 pi 和 mu，前提是全局参数有效\n",
    "                global_pi = global_param[0]\n",
    "                global_mu = global_param[2]\n",
    "\n",
    "                if not np.isinf(global_pi) and not np.isnan(global_pi):\n",
    "                    combined_params['marginal_param1'][i][0] = alpha * global_pi + (1 - alpha) * domain_param[0]\n",
    "                else:\n",
    "                    combined_params['marginal_param1'][i][0] = domain_param[0]\n",
    "                \n",
    "                if not np.isinf(global_mu) and not np.isnan(global_mu):\n",
    "                    combined_params['marginal_param1'][i][2] = alpha * global_mu + (1 - alpha) * domain_param[2]\n",
    "                else:\n",
    "                    combined_params['marginal_param1'][i][2] = domain_param[2]\n",
    "\n",
    "            elif domain_model == 'ZINB':\n",
    "                # 修正 pi, r 和 mu，前提是全局参数有效\n",
    "                global_pi = global_param[0]\n",
    "                global_r = global_param[1]\n",
    "                global_mu = global_param[2]\n",
    "\n",
    "                if not np.isinf(global_pi) and not np.isnan(global_pi):\n",
    "                    combined_params['marginal_param1'][i][0] = alpha * global_pi + (1 - alpha) * domain_param[0]\n",
    "                else:\n",
    "                    combined_params['marginal_param1'][i][0] = domain_param[0]\n",
    "\n",
    "                if not np.isinf(global_r) and not np.isnan(global_r):\n",
    "                    combined_params['marginal_param1'][i][1] = alpha * global_r + (1 - alpha) * domain_param[1]\n",
    "                else:\n",
    "                    combined_params['marginal_param1'][i][1] = domain_param[1]\n",
    "                \n",
    "                if not np.isinf(global_mu) and not np.isnan(global_mu):\n",
    "                    combined_params['marginal_param1'][i][2] = alpha * global_mu + (1 - alpha) * domain_param[2]\n",
    "                else:\n",
    "                    combined_params['marginal_param1'][i][2] = domain_param[2]\n",
    "\n",
    "    return combined_params\n",
    "\n",
    "adata = adata[~adata.obs['Ground Truth'].isna()].copy()\n",
    "\n",
    "\n",
    "unique_ground_truths = adata.obs['Ground Truth'].unique()\n",
    "\n",
    "\n",
    "global_model_params = run_simulation_tissue(adata)\n",
    "\n",
    "\n",
    "split_adatas = {}\n",
    "domain_params = {}\n",
    "\n",
    "for ground_truth in unique_ground_truths:\n",
    "    \n",
    "    mask = adata.obs['Ground Truth'] == ground_truth\n",
    "    \n",
    "    # 创建新的 AnnData 对象\n",
    "    new_adata = ad.AnnData(\n",
    "        X=adata[mask].X.copy(),\n",
    "        obs=adata[mask].obs.copy(),\n",
    "        var=adata.var.copy(),\n",
    "        uns=adata.uns.copy(),\n",
    "        obsm=adata[mask].obsm.copy() if adata.obsm is not None else None,\n",
    "        varm=adata.varm.copy() if adata.varm is not None else None,\n",
    "        layers=adata[mask].layers.copy() if adata.layers is not None else None\n",
    "    )\n",
    "    \n",
    "    \n",
    "    new_adata.obs_names_make_unique()\n",
    "    \n",
    "    split_adatas[ground_truth] = new_adata\n",
    "    \n",
    "    # 拟合 domain 特定模型\n",
    "    domain_params[ground_truth] = run_simulation_domain(new_adata)\n",
    "\n",
    "    print(f\"Ground Truth: {ground_truth}\")\n",
    "    print(f\"Number of cells: {new_adata.shape[0]}\")\n",
    "    print(f\"Number of genes: {new_adata.shape[1]}\")\n",
    "\n",
    "simulated_adatas = {}\n",
    "alpha = 0.5\n",
    "\n",
    "for ground_truth, split_adata in split_adatas.items():\n",
    "    print(f\"Simulating Ground Truth: {ground_truth}\")\n",
    "  \n",
    "    combined_params = combine_models(global_model_params, domain_params[ground_truth], alpha)\n",
    "    simsrt = SimSRT(split_adata, combined_params)\n",
    "    \n",
    "    simulated_simsrt = srtsim_remain_simulate_count(simsrt, split_adata, num_cores=8, verbose=True)\n",
    "    simulated_counts = simulated_simsrt.simCounts\n",
    "    \n",
    "    if simulated_counts.shape != split_adata.shape:\n",
    "        print(f\"Warning: simulated_counts shape {simulated_counts.shape} does not match adata shape {split_adata.shape}\")\n",
    "        if simulated_counts.shape == (split_adata.shape[1], split_adata.shape[0]):\n",
    "            simulated_counts = simulated_counts.T\n",
    "        elif simulated_counts.shape != split_adata.shape:\n",
    "            raise ValueError(\"Cannot adjust simulated_counts shape to match adata shape\")\n",
    "    \n",
    "    simulated_adata = ad.AnnData(\n",
    "        X=simulated_counts,\n",
    "        obs=split_adata.obs.copy(),\n",
    "        var=split_adata.var.copy(),\n",
    "        obsm={'spatial': split_adata.obsm['spatial']}\n",
    "    )\n",
    "\n",
    "    simulated_adata.obs['total_counts'] = simulated_adata.X.sum(axis=1)\n",
    "    simulated_adata.obs['n_genes'] = (simulated_adata.X > 0).sum(axis=1)\n",
    "\n",
    "    simulated_adatas[ground_truth] = simulated_adata\n",
    "\n",
    "\n",
    "merged_simulated_adata = ad.concat(\n",
    "    list(simulated_adatas.values()),\n",
    "    axis=0,\n",
    "    join='outer',\n",
    "    merge='same',\n",
    "    label='Ground Truth',\n",
    "    keys=list(simulated_adatas.keys())\n",
    ")\n",
    "\n",
    "\n",
    "merged_simulated_adata.obs_names_make_unique()\n",
    "\n",
    "print(\"Merged Simulated AnnData:\")\n",
    "print(merged_simulated_adata)\n",
    "print(\"\\nGround Truth distribution:\")\n",
    "print(merged_simulated_adata.obs['Ground Truth'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_simulated_adata.write_h5ad(\"/Users/chen_yiru/Desktop/simulation/data/simulated/domain_156763.h5ad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 扰动模拟测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import anndata\n",
    "\n",
    "\n",
    "\n",
    "def add_perturbations(model_params, perturbation_scale=0.1):\n",
    "    perturbed_model_params = model_params.copy()\n",
    "    perturbed_marginal_param1 = []\n",
    "    for params, model_type in zip(model_params['marginal_param1'], model_params['model_selected']):\n",
    "        perturbation_factor = 1 + perturbation_scale * np.random.choice([-1, 1])\n",
    "        \n",
    "        if model_type == \"Poisson\":\n",
    "            _, _, mu = params\n",
    "            mu_perturbed = max(1e-6, mu * perturbation_factor)\n",
    "            perturbed_marginal_param1.append([0, np.inf, mu_perturbed])\n",
    "\n",
    "        elif model_type == \"NB\":\n",
    "            _, theta, mu = params\n",
    "            theta_perturbed = max(1e-6, theta * perturbation_factor)\n",
    "            mu_perturbed = max(1e-6, mu * perturbation_factor)\n",
    "            perturbed_marginal_param1.append([0, theta_perturbed, mu_perturbed])\n",
    "\n",
    "        elif model_type == \"ZIP\":\n",
    "            pi0, _, mu = params\n",
    "            pi0_perturbed = max(0, min(1, pi0 * perturbation_factor))\n",
    "            mu_perturbed = max(1e-6, mu * perturbation_factor)\n",
    "            perturbed_marginal_param1.append([pi0_perturbed, np.inf, mu_perturbed])\n",
    "        elif model_type == \"ZINB\":\n",
    "            pi0, theta, mu = params\n",
    "            pi0_perturbed = max(0, min(1, pi0 * perturbation_factor))\n",
    "            theta_perturbed = max(1e-6, theta * perturbation_factor)\n",
    "            mu_perturbed = max(1e-6, mu * perturbation_factor)\n",
    "            perturbed_marginal_param1.append([pi0_perturbed, theta_perturbed, mu_perturbed])\n",
    "\n",
    "    perturbed_model_params['marginal_param1'] = perturbed_marginal_param1\n",
    "\n",
    "    return perturbed_model_params\n",
    "\n",
    "\n",
    "def run_simulation_with_perturbation(adata, model_params, perturbation_scale, output_dir):\n",
    "    model_params = run_simulation_tissue(adata)\n",
    "    perturbed_model_params = add_perturbations(model_params, perturbation_scale)\n",
    "    simsrt = SimSRT(adata, perturbed_model_params)\n",
    "    simulated_simsrt_noise = srtsim_remain_simulate_count(simsrt, adata, num_cores=8, verbose=True)\n",
    "    simulated_counts = simulated_simsrt_noise.simCounts\n",
    "\n",
    "    simulated_noise_adata = anndata.AnnData(\n",
    "        X=simulated_counts.T,\n",
    "        obs=adata.obs.copy(),\n",
    "        var=adata.var.copy(),\n",
    "        obsm={'spatial': adata.obsm['spatial']}\n",
    "    )\n",
    "\n",
    "    simulated_noise_adata.obs['total_counts'] = simulated_noise_adata.X.sum(axis=1)\n",
    "    simulated_noise_adata.obs['n_genes'] = (simulated_noise_adata.X > 0).sum(axis=1)\n",
    "    output_path = f\"{output_dir}/simulated_noise_{perturbation_scale}.h5ad\"\n",
    "    simulated_noise_adata.write_h5ad(output_path)\n",
    "    print(f\"Saved perturbed data with scale {perturbation_scale} to {output_path}\")\n",
    "\n",
    "\n",
    "adata = sc.read_h5ad(\"/Users/chen_yiru/Desktop/simulation/Sample_data_151676.h5ad\")\n",
    "output_dir = \"/Users/chen_yiru/Desktop/simulation/results\"\n",
    "perturbation_scales = [0.15]\n",
    "\n",
    "# 运行不同扰动尺度的模拟\n",
    "for scale in perturbation_scales:\n",
    "    run_simulation_with_perturbation(adata, model_params, scale, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference free 新测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simulation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
